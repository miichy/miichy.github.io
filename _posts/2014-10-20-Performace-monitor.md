---
layout: post
title:  "Performace Monitor"
date:   2014-10-20 00:16:00
categories: performace
---

---
# 性能测试监控
本文由前两天的thrift服务性能测试而来，用于记录测试中所习所得。测试服务为thrift服务，数据库为mongodb，施压端为本人自己的工作机器，运用的jmeter施压，与之相同且更出名的是LoadRunner。另：[酷壳](http://coolshell.cn/articles/7490.html)

## 测试之前
服务端机器以及数据库所在的机器，关于防火墙设置，端口最大链接数，tcp链接数，tcp链接Alive时间，资源释放间隔时间等等配置。Jmeter的JVM内存配置为1G。RPC server数以及mongodb的连接池数都是需要调整的地方。

## 监控指标

- 监控的指标：服务器端：cpu，Memory，I/O，Disk
- 数据库：cpu，每秒执行操作次数
- 施压端：请求响应时间以及吞吐量

监控工具为nmon，分析数据为nmon analyser。

## 监控过程

测试之前，观察机器的指标是否处于正常。压力测试启动后，利用监控工具收集性能测试指标数据。测试完成后，查看机器的性能指标是否回归到正常范围，分析收集的性能数据，找出瓶颈，得出意见或者结论。

## 具体实施

本次压力测试为了测试用户服务业务所能承受的压力级别。用户服务是采用的thrift框架的rpc服务，前端php调用rpc接口获取跟用户相关的操作数据。

编写测试case，将读数据接口与写数据接口分开，施压工具采用的jmeter。施压开始时，将sample设置为1000*1000，sample跑到6w时，jmeter返回异常。通过查看tcp链接数，发现是tcp链接达到峰值且没有释放所造成的。这是case的编写bug，由于jmeter的junit request中，不能调用before和after class方法，导致的问题是：rpc服务的链接在哪一个阶段链接？

编写case时，将rpc服务的链接写在构造函数中，则case在第一次跑的时候，即刻open rpc服务，而在后续的loop中，则不用链接服务即可调用服务。则jmeter的1000*1000（1000threads ＊ 1000 Loop）sample即为1000个服务链接打开，且每个链接循环1000次。由于java没有希构函数，导致这些链接不会被释放。但是问题：tcp的链接数最大为65535个，为什么1000个就报错了呢？

为了解决这个异常，case的编写改为每一次test case run的时候就打开一次服务链接，获取数据后则关闭链接，则设置1000*1000samples时，意味着打开了一百万的服务链接。而每一次rpc的链接即为一次tcp链接，需要通过三次握手协议，这当然会对响应时间造成一定的影响。

由于希望服务达到的性能为一秒中能承受的服务链接数，因此将1000\*1000,改为1000\*1.读写数据的接口90%line的响应时间均在100ms以内，但是有一个读数据接口，每次读取数据非常大，给接口大响应时间在300ms左右，最大的可以达到十几秒的。这一点是无法忍受的。

并且这是将jmeter的system.out去掉后才有的结果。在调整rpc server的最小数据为1000，最大为5000，mongodb连接池为240\*10后，数据没有明显上升，且在1000\*1和1000\*10时，mongodb的每秒读写最大为2k，而在mongdb的性能测试中，读写最高峰值时2w和5w，而数据库和服务所在的机器cpu负载，仅仅cpu001的峰值为30%，mem最高为400m。

---

jmeter的响应时间大大多于方法的调用时间（时序图表示），而这两者时间之差为jmeter发出请求到rpc服务所在机器到时间t1加上rpc服务所在机器将数据返回给jmeter的时间t2。而t1则是rpc服务打开链接且进行通信的时间，这包括rpc链接的tcp三次握手时间。t2为tcp close时间，初步分析，t1远大于t2。Jmeter之所以响应时间如此大跟t1成正比关系。

------
## 性能指标

**Performance tuning**
性能调试是一个过程：找到系统的瓶颈，调试操作系统从而消除瓶颈。这个过程中没有简单的“操作步骤、设置参数”，而是获得一种平衡态，操作系统不同的子系统之间的平衡态。

- **CPU** 
- **Memory**
- **IO**
- **Network**

这些子系统是互相依赖，任何之一满负荷均会影响其他：

- **大量的内存页IO请求会填满内存队列**
- **网络的满负荷也会将cpu耗尽**
- **试图保持内存队列空闲时，cpu也会大量被消耗**
- **大量的从内存写磁盘也可能消耗CPU和IO**

### 决定程序类型

为了理解从哪里开始寻找和调试瓶颈，首先时要理解系统的行为。系统的应用程序堆栈通常有以下两种类型：

- **偏重IO** －需要大量使用内存和系统的底层存储。这是由于偏重IO的程序需要处理大量数据，并且不需要很多cpu或网络（除非存储系统非本地）。用cpu资源进行IO请求，然后然后进入睡眠状态。数据型应用程序通常被认为是偏重IO程序。
- **偏重CPU** －对CPU的需求量很大。批量处理and和or等数学计算。高容量等web服务、邮件服务、任何渲染类等服务均是偏重CPU应用程序。 

### 决定统计量的基线

举例：**vmstat**

#### procs

- r:运行队列中等待的进程数
- b:等待io的进程数
- w:可进入运行队列但被替换的进程 

### memory

- swpd:被使用的虚拟内存数
- free:空闲的内存
- buff:使用的内存buff数

### swap
- si:虚拟内存的页导入（SWAP DISK导入RAM）
- so:虚拟内存的页导出

### IO

- bi:写入
- bo:写出

### System
- in:每秒的中断数，包括clock？
- sc:每秒上下文的切换数

### CPU

- us:用户时间
- sy:系统时间
- id:空闲时间

(未完待续)
